{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1cec8d0",
   "metadata": {},
   "source": [
    "Following the Coursera Machine Learning Course, am going to start manually creating the algorithms covered for practice. Below is one feature linear regression fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b18fae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c18fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values of X and y we will perform manual linear regression on\n",
    "X = np.array([0.0, 0.5, 1.0, 1.5, 2.0,\n",
    "              2.5, 3.0, 3.5, 4.0, 4.5, 5.0])\n",
    "\n",
    "y = np.array([2.1, 3.5, 5.1, 6.6, 8.0,\n",
    "              9.6, 11.1, 12.4, 14.1, 15.5, 17.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54e25c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict y given x and the current slope/y-intercept\n",
    "def predict(x, w, b):\n",
    "    return (x * w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa94bec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.01181818181818\n"
     ]
    }
   ],
   "source": [
    "# Gives current standalone cost - not the partial derivative as utilized in gradient descent\n",
    "def cost(X, y, w, b):\n",
    "    examples = X.shape[0]\n",
    "    total = 0\n",
    "    for i in range(examples):\n",
    "        total += (predict(X[i], w, b) - y[i]) ** 2\n",
    "    total /= (2 * examples)\n",
    "    return total\n",
    "\n",
    "curCost = cost(X, y, 0, 0)\n",
    "print(curCost)\n",
    "# ^^ Our cost initially is 57 let us see how gradient descent can refine it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41357a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is 0.3101256198347094\n",
      "Cost is 0.09514220139343962\n",
      "Cost is 0.05699439857636301\n",
      "Cost is 0.034633092100956726\n",
      "Cost is 0.021525442633312568\n",
      "Cost is 0.013842059967116682\n",
      "Cost is 0.009338249475317634\n",
      "Cost is 0.0066982262163847\n",
      "Cost is 0.005150709325876585\n",
      "Cost is 0.004243592875934409\n",
      "Cost is 0.0037118634590842313\n",
      "Cost is 0.003400176715048414\n",
      "Cost is 0.003217473589494159\n",
      "Cost is 0.0031103774979021064\n",
      "Cost is 0.0030476003954333325\n",
      "Cost is 0.003010801997510497\n",
      "Cost is 0.002989231679091368\n",
      "Cost is 0.0029765876874240944\n",
      "Cost is 0.00296917608958977\n",
      "Cost is 0.0029648315927011033\n"
     ]
    }
   ],
   "source": [
    "# Perform gradient descent to fit the model to our dataset - standard looping\n",
    "def gradientDescent(X, y, learningRate, iterations):\n",
    "    examples = X.shape[0]\n",
    "    w = 0 \n",
    "    b = 0 \n",
    "    w_partialD = 0 \n",
    "    b_partialD = 0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Calculate partial derivative w.r.t w and b\n",
    "        for j in range(examples):\n",
    "            w_partialD += (predict(X[j], w, b) - y[j]) * X[j]\n",
    "            b_partialD += (predict(X[j], w, b) - y[j])\n",
    "        w_partialD /= examples\n",
    "        b_partialD /= examples\n",
    "\n",
    "        # Calculate w and b\n",
    "        w = w - (learningRate * w_partialD)\n",
    "        b = b - (learningRate * b_partialD)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Cost is {cost(X, y, w, b)}\")\n",
    "        w_partialD = 0\n",
    "        b_partialD = 0\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "w, b = gradientDescent(X, y, 0.1, 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9749d8c7",
   "metadata": {},
   "source": [
    "At around 200 iterations convergence can be seen, will now try a vectorized implementation to slightly speed up the above gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f34316f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is 0.3101256198347094\n",
      "Cost is 0.09514220139343962\n",
      "Cost is 0.05699439857636301\n",
      "Cost is 0.034633092100956726\n",
      "Cost is 0.021525442633312568\n",
      "Cost is 0.013842059967116682\n",
      "Cost is 0.009338249475317634\n",
      "Cost is 0.0066982262163847\n",
      "Cost is 0.005150709325876585\n",
      "Cost is 0.004243592875934409\n",
      "Cost is 0.0037118634590842313\n",
      "Cost is 0.003400176715048414\n",
      "Cost is 0.003217473589494159\n",
      "Cost is 0.0031103774979021064\n",
      "Cost is 0.0030476003954333325\n",
      "Cost is 0.003010801997510497\n",
      "Cost is 0.002989231679091368\n",
      "Cost is 0.0029765876874240944\n",
      "Cost is 0.00296917608958977\n",
      "Cost is 0.0029648315927011033\n"
     ]
    }
   ],
   "source": [
    "# Perform gradient descent to fit the model to our dataset - vectorized\n",
    "def v_gradientDescent(X, y, learningRate, iterations):\n",
    "    examples = X.shape[0]\n",
    "    w = 0 \n",
    "    b = 0 \n",
    "    dw = 0 \n",
    "    db = 0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_hat = X * w + b\n",
    "        error = y_hat - y\n",
    "\n",
    "        # Calculate partial derivative w.r.t w and b\n",
    "        dw = np.dot(error, X)\n",
    "        db = np.sum(error)\n",
    "        dw /= examples\n",
    "        db /= examples\n",
    "\n",
    "        # Update values of w and b\n",
    "        w -= learningRate * dw\n",
    "        b -= learningRate * db\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Cost is {cost(X, y, w, b)}\")\n",
    "\n",
    "        dw = 0\n",
    "        db = 0\n",
    "    return w, b\n",
    "\n",
    "w, b = v_gradientDescent(X, y, 0.1, 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34128317",
   "metadata": {},
   "source": [
    "Both after convergence give us the same cost but with vectorization we see better timing which on a larger dataset would be more obvious as the numpy functions take better advantage of comptuer hardware to make the matrix operations faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cbfe02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is 4093.9301652892577\n",
      "Cost is 1.5369820164492894e+22\n",
      "Cost is 5.770414371911267e+40\n",
      "Cost is 2.1664327667596213e+59\n",
      "Cost is 8.133611609828282e+77\n",
      "Cost is 3.0536667850756233e+96\n",
      "Cost is 1.146462516479929e+115\n",
      "Cost is 4.304255815065759e+133\n",
      "Cost is 1.6159811468072303e+152\n",
      "Cost is 6.067007118155026e+170\n",
      "Cost is 2.277784950923978e+189\n",
      "Cost is 8.551670010622152e+207\n",
      "Cost is 3.210621790301527e+226\n",
      "Cost is 1.2053893879856398e+245\n",
      "Cost is 4.525489676353123e+263\n",
      "Cost is 1.6990407427597737e+282\n",
      "Cost is 6.378844394766074e+300\n",
      "Cost is inf\n",
      "Cost is inf\n",
      "Cost is inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darth\\AppData\\Local\\Temp\\ipykernel_19532\\3529007109.py:6: RuntimeWarning: overflow encountered in scalar power\n",
      "  total += (predict(X[i], w, b) - y[i]) ** 2\n"
     ]
    }
   ],
   "source": [
    "# We can also experiment with different learning rates\n",
    "w, b = v_gradientDescent(X, y, 1, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed377e4",
   "metadata": {},
   "source": [
    "As opposed to converging we see here that the cost fluctuated greatly showing that this learning rate is too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8628aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is 46.73771365289256\n",
      "Cost is 6.503653727102347\n",
      "Cost is 1.015706436530754\n",
      "Cost is 0.26155032590465405\n",
      "Cost is 0.15261110383055077\n",
      "Cost is 0.13188807993907\n",
      "Cost is 0.12348507925810709\n",
      "Cost is 0.11704727717334279\n",
      "Cost is 0.11114929731378168\n",
      "Cost is 0.10558297343391634\n",
      "Cost is 0.100306678018698\n",
      "Cost is 0.09530215313847443\n",
      "Cost is 0.09055497155633867\n",
      "Cost is 0.08605184169491785\n",
      "Cost is 0.08178020893957332\n",
      "Cost is 0.0777281713919346\n",
      "Cost is 0.07388443999951795\n",
      "Cost is 0.07023830619031463\n",
      "Cost is 0.06677961191438726\n",
      "Cost is 0.063498721326801\n"
     ]
    }
   ],
   "source": [
    "w, b = v_gradientDescent(X, y, 0.01, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a805cf58",
   "metadata": {},
   "source": [
    "Here we see convergence but very slowly which is an indicator that the learning rate needs to be increased. Overall, 0.1 appeared to be a good pick allowing us to reach convergence in about 200 iterations but I included this to have some extra notes on the repo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
