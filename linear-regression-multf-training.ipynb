{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94e9bd8",
   "metadata": {},
   "source": [
    "Will now be fitting linear regression with multiple features and completing all the math/notes with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebaaa3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e835f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix of examples along with vector corresponding to outputs\n",
    "X = np.array([\n",
    "    [1.2, 3.4, 5.1, 2.2],\n",
    "    [2.3, 0.4, 1.1, 3.3],\n",
    "    [0.9, 4.5, 2.2, 1.1],\n",
    "    [3.3, 2.2, 0.5, 4.0],\n",
    "    [1.1, 1.0, 4.7, 2.8],\n",
    "    [2.5, 3.8, 1.4, 0.9],\n",
    "    [4.1, 0.9, 0.8, 2.6],\n",
    "    [3.7, 2.1, 3.3, 1.5],\n",
    "    [0.6, 4.9, 2.7, 3.9],\n",
    "    [1.9, 1.8, 4.6, 2.4],\n",
    "    [2.4, 3.3, 1.6, 1.2],\n",
    "    [3.8, 2.7, 0.9, 3.1],\n",
    "    [0.7, 4.2, 3.0, 2.0],\n",
    "    [1.5, 1.3, 4.1, 3.6],\n",
    "    [2.6, 3.1, 1.8, 0.7],\n",
    "    [3.9, 2.9, 0.6, 1.9],\n",
    "    [0.8, 4.4, 2.5, 2.7],\n",
    "    [1.7, 1.6, 4.3, 1.8],\n",
    "    [2.9, 3.6, 1.3, 0.6],\n",
    "    [3.4, 2.4, 0.7, 3.4]\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    10.2, 5.3, 8.7, 6.1, 9.5,\n",
    "    7.2, 4.8, 6.9, 9.1, 8.6,\n",
    "    7.4, 6.3, 8.4, 9.0, 7.0,\n",
    "    6.5, 8.2, 8.9, 7.1, 6.7\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de8ef543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is 29.838750000000005\n"
     ]
    }
   ],
   "source": [
    "# Returns the cost of the given LR model when passed w and b\n",
    "def cost(X, y, W, b):\n",
    "    m = X.shape[0]\n",
    "    y_hat = np.dot(X, W) + b\n",
    "    error = y_hat - y\n",
    "\n",
    "    total_cost = (np.sum(error ** 2)) / (2 * m)\n",
    "    return total_cost\n",
    "\n",
    "W = np.array([0, 0, 0, 0])\n",
    "b = 0\n",
    "\n",
    "print(f\"Cost is {cost(X, y, W, b)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b426291",
   "metadata": {},
   "source": [
    "With default values of 0 for the weights and intercept we see a cost of 29.8, after gradient descent lets see how this can be improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f42e4aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is currently 64.13325610110564\n",
      "Cost is currently 363146561.1037983\n",
      "Cost is currently 2069093840421378.5\n",
      "Cost is currently 1.1789039967585798e+22\n",
      "Cost is currently 6.717020786695265e+28\n",
      "Cost is currently 3.827145244477089e+35\n",
      "Cost is currently 2.180585885834343e+42\n",
      "Cost is currently 1.2424286254517699e+49\n",
      "Cost is currently 7.07896395812607e+55\n",
      "Cost is currently 4.0333689753990114e+62\n",
      "Cost is currently 2.2980856221250933e+69\n",
      "Cost is currently 1.3093762457216321e+76\n",
      "Cost is currently 7.460410249095376e+82\n",
      "Cost is currently 4.2507049648004683e+89\n",
      "Cost is currently 2.421916770591843e+96\n",
      "Cost is currently 1.3799313036889065e+103\n",
      "Cost is currently 7.862410575055517e+109\n",
      "Cost is currently 4.479751990949914e+116\n",
      "Cost is currently 2.5524204960865976e+123\n",
      "Cost is currently 1.4542881842576102e+130\n",
      "Cost is currently 8.286072479491396e+136\n",
      "Cost is currently 4.721141096971365e+143\n",
      "Cost is currently 2.6899563469519783e+150\n",
      "Cost is currently 1.532651746661219e+157\n",
      "Cost is currently 8.732563185292545e+163\n",
      "Cost is currently 4.975537329419354e+170\n",
      "Cost is currently 2.8349032456060234e+177\n",
      "Cost is currently 1.6152378888664543e+184\n",
      "Cost is currently 9.203112810546855e+190\n",
      "Cost is currently 5.2436415705361206e+197\n",
      "Cost is currently 2.987660532504188e+204\n",
      "Cost is currently 1.7022741423896709e+211\n",
      "Cost is currently 9.69901775761545e+217\n",
      "Cost is currently 5.5261924692389384e+224\n",
      "Cost is currently 3.148649066354662e+231\n",
      "Cost is currently 1.7940002991646503e+238\n",
      "Cost is currently 1.0221644284826528e+245\n",
      "Cost is currently 5.823968476157854e+251\n",
      "Cost is currently 3.3183123836182314e+258\n",
      "Cost is currently 1.89066907218879e+265\n",
      "Cost is currently 1.0772432270627577e+272\n",
      "Cost is currently 6.137789988330164e+278\n",
      "Cost is currently 3.497117920487176e+285\n",
      "Cost is currently 1.9925467917679206e+292\n",
      "Cost is currently 1.1352899180567354e+299\n",
      "Cost is currently 6.46852160946087e+305\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently inf\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n",
      "Cost is currently nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darth\\AppData\\Local\\Temp\\ipykernel_18708\\2650675279.py:7: RuntimeWarning: overflow encountered in square\n",
      "  total_cost = (np.sum(error ** 2)) / (2 * m)\n",
      "C:\\Users\\darth\\AppData\\Local\\Temp\\ipykernel_18708\\2103423673.py:13: RuntimeWarning: overflow encountered in dot\n",
      "  dw = (np.dot(X.T, error)) / m\n",
      "C:\\Users\\darth\\AppData\\Local\\Temp\\ipykernel_18708\\2103423673.py:16: RuntimeWarning: invalid value encountered in subtract\n",
      "  W = W - (learningRate * dw)\n"
     ]
    }
   ],
   "source": [
    "# Immediately going to vectorized gradient descent this time - without feature scaling\n",
    "def v_gradientDescent(X, y, learningRate, iterations):\n",
    "    m = X.shape[0]\n",
    "    W = np.array([0, 0, 0, 0])\n",
    "    b = 0\n",
    "    dw = 0\n",
    "    db = 0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_hat = np.dot(X, W) + b\n",
    "        error = y_hat - y\n",
    "\n",
    "        dw = (np.dot(X.T, error)) / m\n",
    "        db = (np.sum(error)) / m\n",
    "\n",
    "        W = W - (learningRate * dw)\n",
    "        b = b - (learningRate * db)\n",
    "\n",
    "        if (i % 20 == 0):\n",
    "            print(f\"Cost is currently {cost(X, y, W, b)}\")\n",
    "\n",
    "    return W, b\n",
    "\n",
    "w, b = v_gradientDescent(X, y, 0.1, 4000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1908922",
   "metadata": {},
   "source": [
    "With all of our values already being closely correlated in the input data, it is unlikely that regularization will make significant difference in the outcome but we will test that along with a different learning rate. As we can see 0.1 causes divergence and our cost is not converging at 0 but rather fluctuating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3b9d50cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is currently 28.563105960707606\n",
      "Cost is currently 12.893217294884915\n",
      "Cost is currently 5.921779782993663\n",
      "Cost is currently 2.777958372558533\n",
      "Cost is currently 1.3589064471512906\n",
      "Cost is currently 0.7172536010764751\n",
      "Cost is currently 0.4260401498789066\n",
      "Cost is currently 0.2928367797806179\n",
      "Cost is currently 0.23091468759544007\n",
      "Cost is currently 0.20118354324737808\n",
      "Cost is currently 0.18602339333335044\n",
      "Cost is currently 0.17749089716436064\n",
      "Cost is currently 0.17200396432284215\n",
      "Cost is currently 0.1679461586228137\n",
      "Cost is currently 0.16458701869664077\n",
      "Cost is currently 0.16159545229530672\n",
      "Cost is currently 0.15882042448883726\n",
      "Cost is currently 0.1561921132766871\n",
      "Cost is currently 0.15367735902145852\n",
      "Cost is currently 0.15125958377044713\n",
      "Cost is currently 0.14892973904127216\n",
      "Cost is currently 0.14668222412153148\n",
      "Cost is currently 0.14451304460804257\n",
      "Cost is currently 0.14241898066940886\n",
      "Cost is currently 0.14039721044537054\n",
      "Cost is currently 0.13844513863285965\n",
      "Cost is currently 0.13656031760608162\n",
      "Cost is currently 0.1347404102969356\n",
      "Cost is currently 0.13298317195051762\n",
      "Cost is currently 0.13128644043964757\n",
      "Cost is currently 0.1296481304872461\n",
      "Cost is currently 0.12806622969858306\n",
      "Cost is currently 0.12653879545617874\n",
      "Cost is currently 0.12506395224883698\n",
      "Cost is currently 0.12363988924011722\n",
      "Cost is currently 0.1222648579869926\n",
      "Cost is currently 0.12093717026701345\n",
      "Cost is currently 0.11965519599378258\n",
      "Cost is currently 0.11841736121028848\n",
      "Cost is currently 0.11722214615407149\n",
      "Cost is currently 0.11606808339024537\n",
      "Cost is currently 0.11495375600935513\n",
      "Cost is currently 0.11387779588753752\n",
      "Cost is currently 0.11283888200669716\n",
      "Cost is currently 0.11183573883257318\n",
      "Cost is currently 0.11086713474867158\n",
      "Cost is currently 0.10993188054413046\n",
      "Cost is currently 0.10902882795365317\n",
      "Cost is currently 0.10815686824771556\n",
      "Cost is currently 0.10731493087131341\n",
      "Cost is currently 0.10650198212958206\n",
      "Cost is currently 0.10571702391866653\n",
      "Cost is currently 0.1049590925002957\n",
      "Cost is currently 0.10422725731854281\n",
      "Cost is currently 0.10352061985732955\n",
      "Cost is currently 0.10283831253726641\n",
      "Cost is currently 0.10217949765047474\n",
      "Cost is currently 0.1015433663320859\n",
      "Cost is currently 0.10092913756715025\n",
      "Cost is currently 0.10033605723174113\n",
      "Cost is currently 0.099763397167073\n",
      "Cost is currently 0.09921045428549964\n",
      "Cost is currently 0.09867654970729256\n",
      "Cost is currently 0.09816102792714143\n",
      "Cost is currently 0.09766325600935333\n",
      "Cost is currently 0.09718262281075997\n",
      "Cost is currently 0.09671853823038311\n",
      "Cost is currently 0.0962704324849334\n",
      "Cost is currently 0.0958377554092549\n",
      "Cost is currently 0.0954199757808549\n",
      "Cost is currently 0.09501658066769299\n",
      "Cost is currently 0.09462707479842375\n",
      "Cost is currently 0.09425097995432391\n",
      "Cost is currently 0.09388783438215624\n",
      "Cost is currently 0.0935371922272483\n",
      "Cost is currently 0.09319862298609058\n",
      "Cost is currently 0.09287171097778361\n",
      "Cost is currently 0.09255605483368204\n",
      "Cost is currently 0.09225126700461075\n",
      "Cost is currently 0.0919569732850499\n",
      "Cost is currently 0.09167281235370014\n",
      "Cost is currently 0.091398435329868\n",
      "Cost is currently 0.09113350534512529\n",
      "Cost is currently 0.09087769712971612\n",
      "Cost is currently 0.09063069661320476\n",
      "Cost is currently 0.09039220053887287\n",
      "Cost is currently 0.09016191609139468\n",
      "Cost is currently 0.08993956053732965\n",
      "Cost is currently 0.08972486087799544\n",
      "Cost is currently 0.08951755351429112\n",
      "Cost is currently 0.08931738392306164\n",
      "Cost is currently 0.08912410634460512\n",
      "Cost is currently 0.08893748348093974\n",
      "Cost is currently 0.08875728620445848\n",
      "Cost is currently 0.08858329327661743\n",
      "Cost is currently 0.08841529107630777\n",
      "Cost is currently 0.08825307333758046\n",
      "Cost is currently 0.08809644089640216\n",
      "Cost is currently 0.08794520144613052\n",
      "Cost is currently 0.08779916930140728\n",
      "Cost is currently 0.08765816517018254\n",
      "Cost is currently 0.0875220159335874\n",
      "Cost is currently 0.08739055443338557\n",
      "Cost is currently 0.08726361926674395\n",
      "Cost is currently 0.0871410545880694\n",
      "Cost is currently 0.08702270991766774\n",
      "Cost is currently 0.08690843995699082\n",
      "Cost is currently 0.08679810441024643\n",
      "Cost is currently 0.08669156781214739\n",
      "Cost is currently 0.08658869936159455\n",
      "Cost is currently 0.08648937276108389\n",
      "Cost is currently 0.08639346606164422\n",
      "Cost is currently 0.08630086151311314\n",
      "Cost is currently 0.08621144541956857\n",
      "Cost is currently 0.08612510799973787\n",
      "Cost is currently 0.0860417432522126\n",
      "Cost is currently 0.08596124882530524\n",
      "Cost is currently 0.08588352589138515\n",
      "Cost is currently 0.08580847902554364\n",
      "Cost is currently 0.08573601608843338\n",
      "Cost is currently 0.0856660481131456\n",
      "Cost is currently 0.08559848919597911\n",
      "Cost is currently 0.08553325639097185\n",
      "Cost is currently 0.0854702696080628\n",
      "Cost is currently 0.08540945151476179\n",
      "Cost is currently 0.08535072744120314\n",
      "Cost is currently 0.0852940252884696\n",
      "Cost is currently 0.08523927544007198\n",
      "Cost is currently 0.0851864106764775\n",
      "Cost is currently 0.0851353660925802\n",
      "Cost is currently 0.085086079018013\n",
      "Cost is currently 0.08503848894020434\n",
      "Cost is currently 0.08499253743008288\n",
      "Cost is currently 0.08494816807034059\n",
      "Cost is currently 0.08490532638616678\n",
      "Cost is currently 0.0848639597783657\n",
      "Cost is currently 0.08482401745877731\n",
      "Cost is currently 0.08478545038792219\n",
      "Cost is currently 0.08474821121479284\n",
      "Cost is currently 0.08471225421871781\n",
      "Cost is currently 0.08467753525322827\n",
      "Cost is currently 0.08464401169185662\n",
      "Cost is currently 0.08461164237580135\n",
      "Cost is currently 0.08458038756339438\n",
      "Cost is currently 0.08455020888130782\n",
      "Cost is currently 0.0845210692774405\n",
      "Cost is currently 0.08449293297542734\n",
      "Cost is currently 0.084465765430715\n",
      "Cost is currently 0.08443953328814831\n",
      "Cost is currently 0.0844142043410197\n",
      "Cost is currently 0.08438974749152629\n",
      "Cost is currently 0.08436613271258939\n",
      "Cost is currently 0.08434333101098887\n",
      "Cost is currently 0.08432131439176631\n",
      "Cost is currently 0.08430005582385408\n",
      "Cost is currently 0.08427952920688742\n",
      "Cost is currently 0.08425970933915998\n",
      "Cost is currently 0.08424057188668205\n",
      "Cost is currently 0.08422209335330409\n",
      "Cost is currently 0.08420425105186964\n",
      "Cost is currently 0.08418702307636099\n",
      "Cost is currently 0.08417038827500274\n",
      "Cost is currently 0.08415432622429428\n",
      "Cost is currently 0.08413881720393353\n",
      "Cost is currently 0.08412384217260552\n",
      "Cost is currently 0.08410938274460453\n",
      "Cost is currently 0.08409542116726045\n",
      "Cost is currently 0.08408194029914293\n",
      "Cost is currently 0.08406892358901594\n",
      "Cost is currently 0.08405635505551722\n",
      "Cost is currently 0.08404421926753636\n",
      "Cost is currently 0.08403250132526939\n",
      "Cost is currently 0.08402118684192596\n",
      "Cost is currently 0.08401026192606503\n",
      "Cost is currently 0.08399971316453995\n",
      "Cost is currently 0.08398952760603047\n",
      "Cost is currently 0.08397969274514154\n",
      "Cost is currently 0.08397019650704972\n",
      "Cost is currently 0.08396102723267793\n",
      "Cost is currently 0.08395217366438186\n",
      "Cost is currently 0.08394362493212701\n",
      "Cost is currently 0.08393537054014404\n",
      "Cost is currently 0.08392740035404185\n",
      "Cost is currently 0.08391970458836524\n",
      "Cost is currently 0.08391227379458069\n",
      "Cost is currently 0.08390509884947624\n",
      "Cost is currently 0.08389817094395954\n",
      "Cost is currently 0.08389148157224322\n",
      "Cost is currently 0.08388502252140138\n",
      "Cost is currently 0.08387878586128643\n",
      "Cost is currently 0.08387276393479277\n",
      "Cost is currently 0.08386694934845526\n",
      "Cost is currently 0.08386133496337247\n",
      "Cost is currently 0.08385591388644116\n",
      "Cost is currently 0.08385067946189413\n",
      "Cost is currently 0.08384562526312789\n",
      "Cost is currently 0.08384074508481218\n",
      "Cost is currently 0.08383603293527177\n",
      "Cost is currently 0.0838314830291303\n",
      "Cost is currently 0.0838270897802065\n"
     ]
    }
   ],
   "source": [
    "w, b = v_gradientDescent(X, y, 0.08, 4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f659b",
   "metadata": {},
   "source": [
    "With an adjusted learning rate of 0.8, we can now see the data converging closer to 0. Below gradient descent will be re-implemented but with feature scaling for the sake of trying to get an even lower cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa9b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.93110806  0.53051432  1.83385315 -0.08159227]\n",
      " [ 0.0305998  -1.82732712 -0.84330473  0.97430772]\n",
      " [-1.19339202  1.39505619 -0.10708632 -1.13749226]\n",
      " [ 0.90487966 -0.41262225 -1.24487842  1.64624408]\n",
      " [-1.01853605 -1.35575883  1.56613736  0.49435318]\n",
      " [ 0.20545577  0.84489318 -0.64251789 -1.32947408]\n",
      " [ 1.60430356 -1.43435354 -1.04409158  0.30237136]\n",
      " [ 1.25459161 -0.49121697  0.6291321  -0.75352863]\n",
      " [-1.45567598  1.70943504  0.22755842  1.55025317]\n",
      " [-0.31911215 -0.72700111  1.49920842  0.11038954]\n",
      " [ 0.11802778  0.45191961 -0.50866    -1.04150135]\n",
      " [ 1.3420196  -0.01964868 -0.97716263  0.7823259 ]\n",
      " [-1.368248    1.15927204  0.42834526 -0.27357409]\n",
      " [-0.6688241  -1.11997468  1.16456368  1.26228044]\n",
      " [ 0.29288376  0.29473018 -0.3748021  -1.52145589]\n",
      " [ 1.42944759  0.13754075 -1.17794947 -0.369565  ]\n",
      " [-1.28082001  1.31646147  0.09370053  0.39836227]\n",
      " [-0.49396813 -0.88419054  1.29842157 -0.4655559 ]\n",
      " [ 0.55516772  0.68770375 -0.70944684 -1.6174468 ]\n",
      " [ 0.99230765 -0.25543282 -1.11102052  1.07029863]]\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling our input matrix X - ZScore Normalization\n",
    "X_means = np.zeros(X.shape[1])\n",
    "X_stds = np.zeros(X.shape[1])\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    X_means[i] = np.mean(X[:, i])\n",
    "    X_stds[i] = np.std(X[:, i])\n",
    "\n",
    "X_scaled = (X - X_means) / X_stds\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e79d0",
   "metadata": {},
   "source": [
    "Now these scaled X values hover around 0 and we can see how this impacts gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7638a41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is currently 29.234955272815984\n",
      "Cost is currently 19.453554889450924\n",
      "Cost is currently 12.9803531321376\n",
      "Cost is currently 8.684770296207436\n",
      "Cost is currently 5.828265892276974\n",
      "Cost is currently 3.92565137277942\n",
      "Cost is currently 2.6567773667931243\n",
      "Cost is currently 1.8096824446297846\n",
      "Cost is currently 1.2436726241515563\n",
      "Cost is currently 0.8651771826076221\n",
      "Cost is currently 0.6118736556874358\n",
      "Cost is currently 0.4422060681358465\n",
      "Cost is currently 0.3284419015315646\n",
      "Cost is currently 0.252061843813704\n",
      "Cost is currently 0.20069286258802332\n",
      "Cost is currently 0.16606510864379226\n",
      "Cost is currently 0.14264943253308976\n",
      "Cost is currently 0.12674806362737975\n",
      "Cost is currently 0.11588736855011339\n",
      "Cost is currently 0.10841215571569725\n",
      "Cost is currently 0.1032145404557427\n",
      "Cost is currently 0.0995526934786113\n",
      "Cost is currently 0.0969296519197925\n",
      "Cost is currently 0.095012277287499\n",
      "Cost is currently 0.0935770540318834\n",
      "Cost is currently 0.09247383559473657\n",
      "Cost is currently 0.09160159285794947\n",
      "Cost is currently 0.09089218997956913\n",
      "Cost is currently 0.09029952947823916\n",
      "Cost is currently 0.08979228885275738\n",
      "Cost is currently 0.08934905974229664\n",
      "Cost is currently 0.08895509433934227\n",
      "Cost is currently 0.0886001270819136\n",
      "Cost is currently 0.08827691577036806\n",
      "Cost is currently 0.0879802640575836\n",
      "Cost is currently 0.08770636606161503\n",
      "Cost is currently 0.08745236656211731\n",
      "Cost is currently 0.08721606550370105\n",
      "Cost is currently 0.08699571911837557\n",
      "Cost is currently 0.08678990575977712\n",
      "Cost is currently 0.08659743509894366\n",
      "Cost is currently 0.08641728639414697\n",
      "Cost is currently 0.0862485662724357\n",
      "Cost is currently 0.08609047962186907\n",
      "Cost is currently 0.08594230930856687\n",
      "Cost is currently 0.08580340184795476\n",
      "Cost is currently 0.08567315710659529\n",
      "Cost is currently 0.08555102074475333\n",
      "Cost is currently 0.0854364785340113\n",
      "Cost is currently 0.08532905196819844\n"
     ]
    }
   ],
   "source": [
    "w, b = v_gradientDescent(X_scaled, y, 0.01, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a7c04",
   "metadata": {},
   "source": [
    "Where this learning rate was too large for the non-feature scaled matrix, it actually begins to convege with the scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b51b622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is currently 25.184317234044496\n",
      "Cost is currently 0.9768250824346196\n",
      "Cost is currently 0.12772738507423903\n",
      "Cost is currently 0.09260685412899422\n",
      "Cost is currently 0.08840445709687596\n",
      "Cost is currently 0.08649092913843563\n",
      "Cost is currently 0.08536868364113018\n",
      "Cost is currently 0.08469877057661945\n",
      "Cost is currently 0.08429844591059976\n",
      "Cost is currently 0.08405920579476235\n",
      "Cost is currently 0.08391623170920856\n",
      "Cost is currently 0.08383078787031242\n",
      "Cost is currently 0.08377972512224326\n",
      "Cost is currently 0.08374920912159896\n",
      "Cost is currently 0.0837309722203236\n",
      "Cost is currently 0.08372007352583209\n",
      "Cost is currently 0.08371356027339803\n",
      "Cost is currently 0.08370966783863872\n",
      "Cost is currently 0.08370734165083808\n",
      "Cost is currently 0.08370595147989912\n",
      "Cost is currently 0.08370512068917832\n",
      "Cost is currently 0.08370462419396159\n",
      "Cost is currently 0.08370432747964567\n",
      "Cost is currently 0.08370415015792694\n",
      "Cost is currently 0.0837040441873342\n"
     ]
    }
   ],
   "source": [
    "w, b = v_gradientDescent(X_scaled, y, 0.08, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432bc110",
   "metadata": {},
   "source": [
    "Using our learning rate of 0.8 again, convergence was able to be located much faster than previously with only around 500 iterations needed to see convergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
